{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599335996058",
   "display_name": "Python 3.6.9 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary\n",
    "\n",
    "*model*, a specification of a mathematical (or probabilistic) relationship that exists between different variables. Example: a *business model (likely in a spreadsheet) that takes inputs like “number of users” and “ad revenue per user” and “number of employees” and outputs your annual profit for the next several years.\n",
    "\n",
    "*machine learning*, creating and using models that are learned from data. In other contexts this might becalled predictive modeling or data mining. Typically, our goal will be to use existing data to develop models that we can use to predict various outcomes for new data,\n",
    "\n",
    "*supervised* models, models in which there is a set of data labeled with the correct answers to learn from.\n",
    "\n",
    "*unsupervised* models, models in which there are no such labels.\n",
    "\n",
    "*semisupervised* models, models in which only some of the data are labeled.\n",
    "\n",
    "*online* models, models in which the model needs to continuously adjust to\n",
    "newly arriving data.\n",
    "\n",
    "*parameterized* family of models and then use data to learn parameters that are in some way optimal.\n",
    "\n",
    "*overfitting*, producing a model that performs well on the data you train it on but that generalizes poorly to any new data.\n",
    "\n",
    "*noise* in the data\n",
    "\n",
    "*underfitting*, producing a model that doesn’t perform well even on the training data\n",
    "\n",
    "*fitting*\n",
    "\n",
    "*identify*\n",
    "\n",
    "*attributes*\n",
    "\n",
    "*choose*\n",
    "\n",
    "three parts: a *training* set for building models, a *validation* set for choosing among trained models, and a *test* set for judging the final model.\n",
    "\n",
    "*binary* judgment\n",
    "\n",
    "*confusion matrix*\n",
    "\n",
    "*accuracy* is defined as the fraction of correct predictions\n",
    "\n",
    "*precision*, measures how accurate our positive predictions were\n",
    "\n",
    "*recall*, measures what fraction of the positives our model identified\n",
    "\n",
    "*F1 score*\n",
    "\n",
    "*harmonic mean* of precision and recall and necessarily lies between them\n",
    "\n",
    "*bias*, *high bias*, model with a lot of mistakes for pretty much any training set (drawn from the same population)\n",
    "\n",
    "*variance*, *low variance*\n",
    "\n",
    "*features* are whatever inputs we provide to our model\n",
    "\n",
    "*domain expertise*\n"
   ]
  },
  {
   "source": [
    "data science is mostly turning business problems into data problems"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "What is a model? It’s simply a specification of a mathematical (or probabilistic) relationship that exists between different variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning\n",
    "\n",
    "Everyone has her own exact definition, but we’ll use *machine learning* to refer to creating and using models that are *learned from data*. In other contexts this might be called *predictive modeling* or *data mining*, but we will stick with machine learning.\n",
    "\n",
    "Typically, our goal will be to use existing data to develop models that we can use to *predict* various outcomes for new data, such as:\n",
    "\n",
    "* Predicting whether an email message is spam or not\n",
    "* Predicting whether a credit card transaction is fraudulent\n",
    "* Predicting which advertisement a shopper is most likely to click on\n",
    "* Predicting which football team is going to win the Super Bowl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting\n",
    "\n",
    "### How do we make sure our models aren’t too complex? \n",
    "\n",
    "The most fundamental approach involves using different data to train the\n",
    "model and to test the model.\n",
    "\n",
    "The simplest way to do this is to split your data set, so that (for example) two-thirds of it is used to train the model, after which we measure the model’s performance on the remaining third."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correctness\n",
    "\n",
    "We don’t typically use “accuracy” to measure how good a model is.\n",
    "\n",
    "Given a set of labeled data and such a predictive model, every data point lies in one of four categories:\n",
    "* True positive: “This message is spam, and we correctly predicted spam.”\n",
    "* False positive (Type 1 Error): “This message is not spam, but we predicted spam.”\n",
    "* False negative (Type 2 Error): “This message is spam, but we predicted not\n",
    "spam.”\n",
    "* True negative: “This message is not spam, and we correctly predicted not spam.”\n",
    "\n",
    "We often represent these as counts in a *confusion matrix*:\n",
    "\n",
    "| | Spam | not Spam |\n",
    "|:-------------------|:---------------|:---------------|\n",
    "| predict “Spam”     | True Positive  | False Positive |\n",
    "| predict “Not Spam” | False Negative | True Negative  |\n",
    "\n",
    "Usually the choice of a model involves a trade-off between precision and recall. A model that predicts “yes” when it’s even a little bit confident will probably have a high recall but a low precision; a model that predicts “yes” only when it’s extremely confident is likely to have a low recall and a high precision.\n",
    "\n",
    "Alternatively, you can think of this as a trade-off between false positives and false negatives. Saying “yes” too often will give you lots of false positives; saying “no” too often will give you lots of false negatives."
   ]
  },
  {
   "source": [
    "# The Bias-Variance Trade-off\n",
    "\n",
    "The degree 0 model in “Overfitting and Underfitting” will make a lot of mistakes for pretty much any training set (drawn from the same population), which means that it has a high *bias*. However, any two randomly chosen training sets should give pretty similar models (since any two randomly chosen training sets should have pretty similar average values). So we say that it has a low *variance*. High bias and low variance typically correspond to underfitting.\n",
    "\n",
    "On the other hand, the degree 9 model fit the training set perfectly. It has very low bias but very high variance (since any two training sets would likely give rise to very different models). This corresponds to overfitting.\n",
    "\n",
    "If your model has high bias (which means it performs poorly even on your training data) then one thing to try is adding more features. Going from the degree 0 model to the degree 1 model was a big improvement.\n",
    "\n",
    "If your model has high variance, then you can similarly remove features. But another solution is to obtain more data (if you can).\n",
    "\n",
    "$\\hat{Y} = \\hat{\\beta}_{0} + \\sum \\limits _{j=1} ^{p} X_{j}\\hat{\\beta}_{j}$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and Selection\n",
    "\n",
    "As we mentioned, when your data doesn’t have enough features, your model is likely to underfit. And when your data has too many features it’s easy to overfit. But what are features and where do they come from?\n",
    "\n",
    "*Features* are whatever inputs we provide to our model.\n",
    "\n",
    "\n",
    "## Feature types\n",
    "The first is simply a yes or no, which we typically encode as a 1 or 0. The second is a number. And the third is a choice from a discrete set of options.\n",
    "\n",
    "The Naive Bayes classifier we’ll build in Chapter 13 is suited to yes-or-no features, like the first one in the preceding list.\n",
    "\n",
    "Regression models, require numeric features (which could include dummy variables that are 0s and 1s).\n",
    "\n",
    "And decision trees, can deal with numeric or categorical data.\n",
    "\n",
    "## How do we choose features? \n",
    "That’s where a combination of *experience* and *domain expertise* comes into play"
   ]
  }
 ]
}